# ğŸ›¡ï¸ DÃ©tection d'Attaques Adversariales pour les modÃ¨les de classification d'images


## ğŸ“Œ Description du Projet
Ce projet vise Ã  dÃ©velopper un pipeline complet pour gÃ©nÃ©rer, dÃ©tecter et se dÃ©fendre contre les attaques adversariales sur des modÃ¨les de classification d'images. Il est particuliÃ¨rement adaptÃ© aux cas dâ€™usage en imagerie mÃ©dicale (IRM), oÃ¹ la sÃ©curitÃ© et la fiabilitÃ© des modÃ¨les sont cruciales.

---

## ğŸ§  Objectifs

- ImplÃ©menter des attaques adversariales populaires : FGSM, PGD, BIM, MIM, Carlini & Wagner  
- Tester la robustesse des modÃ¨les de classification  
- IntÃ©grer des techniques de dÃ©fense : adversarial training, preprocessing, dÃ©tection  
- DÃ©ployer une API Flask pour automatiser le pipeline  
- IntÃ©grer une interface web pour lâ€™expÃ©rimentation utilisateur  

---

## ğŸ§ª Exemples d'Attaques ImplÃ©mentÃ©es

### ğŸ”¹ FGSM (Fast Gradient Sign Method)

**DÃ©finition :**  
Le FGSM est une attaque Ã  un seul pas qui perturbe lÃ©gÃ¨rement lâ€™image originale dans la direction du gradient du modÃ¨le pour maximiser la perte.

**Ã‰quation (format texte) :**  
`x_adv = x + Îµ * sign(âˆ‡_x J(Î¸, x, y))`

---

### ğŸ”¹ PGD (Projected Gradient Descent)

**DÃ©finition :**  
Le PGD est une version itÃ©rative et plus puissante de FGSM, qui applique plusieurs perturbations tout en projetant lâ€™image modifiÃ©e dans un voisinage autorisÃ©.

**Ã‰quation (format texte) :**  
`x_adv(t+1) = Projection_epsilon(x_adv(t) + Î± * sign(âˆ‡_x J(Î¸, x_adv(t), y)))`

---

### ğŸ”¹ BIM (Basic Iterative Method)

**DÃ©finition :**  
Le BIM est similaire au PGD mais sans projection explicite. Il applique plusieurs petites perturbations successives.

**Ã‰quation (format texte) :**  
`x_adv(t+1) = Clip_{x, Îµ}(x_adv(t) + Î± * sign(âˆ‡_x J(Î¸, x_adv(t), y)))`

---

### ğŸ”¹ MIM (Momentum Iterative Method)

**DÃ©finition :**  
Le MIM introduit un terme de momentum dans lâ€™attaque itÃ©rative, ce qui permet de stabiliser la direction du gradient et dâ€™amÃ©liorer la transfÃ©rabilitÃ© de lâ€™attaque vers dâ€™autres modÃ¨les.

**Ã‰quation (format texte) :**  
```
g(t+1) = Î¼ * g(t) + âˆ‡_x J(Î¸, x_adv(t), y) / L1_norm(âˆ‡_x J(Î¸, x_adv(t), y))
x_adv(t+1) = Clip_{x, Îµ}(x_adv(t) + Î± * sign(g(t+1)))
```

---

### ğŸ”¹ Carlini & Wagner (C&W)

**DÃ©finition :**  
Lâ€™attaque C&W repose sur une optimisation qui minimise la perturbation tout en sâ€™assurant que lâ€™image est mal classÃ©e.

**Ã‰quation (format texte) :**  
```
minimize ||Î´||Â² + c * f(x + Î´)
subject to x + Î´ âˆˆ [0,1]^n
```

---

## ğŸ“Š ModÃ¨le UtilisÃ©

- ModÃ¨le : CNN SÃ©quentiel (Keras)  
- Accuracy sur donnÃ©es propres : **92.5 %**

---
<center><img width="512" height="852" alt="interfac web complet" src="https://github.com/user-attachments/assets/1ae09678-5b87-45f3-b428-c48a89d06948" /></center>


## ğŸ‘©â€ğŸ’» Auteur

**Nour Elwoujoud KhÃ©miri**  
ğŸ“§ khemirinour334@gmail.com  
ğŸ“ FacultÃ© des Sciences de Sfax â€” MastÃ¨re professionnel en CybersÃ©curitÃ© et Industrie Intelligente  
ğŸ”— [LinkedIn](https://www.linkedin.com/in/nour-elwoujoud-khemiri-0463a3209/)
