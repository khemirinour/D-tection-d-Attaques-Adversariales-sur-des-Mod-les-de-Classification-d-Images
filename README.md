# ğŸ›¡ï¸ DÃ©tection d'Attaques Adversariales pour les modÃ¨les de classification d'images


## ğŸ“Œ Description du Projet
Ce projet vise Ã  dÃ©velopper un pipeline complet pour gÃ©nÃ©rer, dÃ©tecter et se dÃ©fendre contre les attaques adversariales sur des modÃ¨les de classification d'images. Il est particuliÃ¨rement adaptÃ© aux cas dâ€™usage en imagerie mÃ©dicale (IRM), oÃ¹ la sÃ©curitÃ© et la fiabilitÃ© des modÃ¨les sont cruciales.

---

## ğŸ§  Objectifs

- ImplÃ©menter des attaques adversariales populaires : FGSM, PGD, BIM, MIM, Carlini & Wagner  
- Tester la robustesse des modÃ¨les de classification  
- IntÃ©grer des techniques de dÃ©fense : adversarial training, preprocessing, dÃ©tection  
- DÃ©ployer une API Flask pour automatiser le pipeline  
- IntÃ©grer une interface web pour lâ€™expÃ©rimentation utilisateur  

---

## ğŸ§ª Exemples d'Attaques ImplÃ©mentÃ©es

### ğŸ”¹ FGSM (Fast Gradient Sign Method)

**DÃ©finition :**  
Le FGSM est une attaque Ã  un seul pas qui perturbe lÃ©gÃ¨rement lâ€™image originale dans la direction du gradient du modÃ¨le pour maximiser la perte.

**Ã‰quation (format texte) :**  
`x_adv = x + Îµ * sign(âˆ‡_x J(Î¸, x, y))`

---

### ğŸ”¹ PGD (Projected Gradient Descent)

**DÃ©finition :**  
Le PGD est une version itÃ©rative et plus puissante de FGSM, qui applique plusieurs perturbations tout en projetant lâ€™image modifiÃ©e dans un voisinage autorisÃ©.

**Ã‰quation (format texte) :**  
`x_adv(t+1) = Projection_epsilon(x_adv(t) + Î± * sign(âˆ‡_x J(Î¸, x_adv(t), y)))`

---

### ğŸ”¹ BIM (Basic Iterative Method)

**DÃ©finition :**  
Le BIM est similaire au PGD mais sans projection explicite. Il applique plusieurs petites perturbations successives.

**Ã‰quation (format texte) :**  
`x_adv(t+1) = Clip_{x, Îµ}(x_adv(t) + Î± * sign(âˆ‡_x J(Î¸, x_adv(t), y)))`

---

### ğŸ”¹ MIM (Momentum Iterative Method)

**DÃ©finition :**  
Le MIM introduit un terme de momentum dans lâ€™attaque itÃ©rative, ce qui permet de stabiliser la direction du gradient et dâ€™amÃ©liorer la transfÃ©rabilitÃ© de lâ€™attaque vers dâ€™autres modÃ¨les.

**Ã‰quation (format texte) :**  
```
g(t+1) = Î¼ * g(t) + âˆ‡_x J(Î¸, x_adv(t), y) / L1_norm(âˆ‡_x J(Î¸, x_adv(t), y))
x_adv(t+1) = Clip_{x, Îµ}(x_adv(t) + Î± * sign(g(t+1)))
```

---

### ğŸ”¹ Carlini & Wagner (C&W)

**DÃ©finition :**  
Lâ€™attaque C&W repose sur une optimisation qui minimise la perturbation tout en sâ€™assurant que lâ€™image est mal classÃ©e.

**Ã‰quation (format texte) :**  
```
minimize ||Î´||Â² + c * f(x + Î´)
subject to x + Î´ âˆˆ [0,1]^n
```
## ğŸ—ï¸ Architecture du Pipeline

Voici lâ€™architecture globale du pipeline utilisÃ© pour la dÃ©tection dâ€™attaques adversariales :

<img width="851" height="772" alt="pipline" src="https://github.com/user-attachments/assets/ddf8bdbc-a15e-4de4-9b3d-6be1ea60fc8a" />


Afin de pallier lâ€™insuffisance de jeux de donnÃ©es publics contenant des exemples dâ€™images mÃ©dicales adversariales, nous avons gÃ©nÃ©rÃ© notre **propre dataset adversarial** Ã  partir dâ€™images IRM dâ€™origine. Celui-ci a Ã©tÃ© utilisÃ© pour lâ€™entraÃ®nement et la validation de notre module de dÃ©tection dâ€™attaques adversariales.

Les Ã©tapes du pipeline sont les suivantes :

1. **PrÃ©traitement des images** : redimensionnement, normalisation.  
2. **GÃ©nÃ©ration dâ€™attaques adversariales** : implÃ©mentation de FGSM, PGD, BIM, MIM, C&W.  
3. **CrÃ©ation du dataset adversarial** : sauvegarde des images perturbÃ©es avec Ã©tiquettes.  
4. **EntraÃ®nement du dÃ©tecteur** : apprentissage supervisÃ© sur des paires (image propre, image attaquÃ©e).  
5. **DÃ©tection conditionnelle** :  
   - Si lâ€™image est dÃ©tectÃ©e **propre**, elle est envoyÃ©e au **classificateur principal** pour prÃ©diction de la classe.  
   - Si lâ€™image est dÃ©tectÃ©e **adversariale**, elle est signalÃ©e ou rejetÃ©e pour Ã©viter une mauvaise classification.  
6. **DÃ©ploiement via API Flask** : automatisation du pipeline.  
7. **Interface web (optionnelle)** :  
   - Permet dâ€™**importer** le dataset dâ€™images propres ainsi que le modÃ¨le de classification, afin de gÃ©nÃ©rer un dataset adversarial personnalisÃ© et dâ€™entraÃ®ner un dÃ©tecteur adaptÃ©.  
   - Offre une **option de test en temps rÃ©el** :  
     - Lâ€™utilisateur peut uploader une image pour analyse immÃ©diate.  
     - Le systÃ¨me utilise le dÃ©tecteur pour dÃ©terminer si lâ€™image est propre ou adversariale.  
     - Si lâ€™image est **propre**, elle est automatiquement classifiÃ©e par le modÃ¨le principal.  
     - Si lâ€™image est **adversariale**, le systÃ¨me affiche le type dâ€™attaque dÃ©tectÃ©, permettant ainsi une meilleure interprÃ©tation et gestion.

---

## ğŸ“Š ModÃ¨le UtilisÃ©

- ModÃ¨le : CNN SÃ©quentiel 
- Accuracy sur donnÃ©es propres : **92.5 %**

---
<img width="512" height="852" alt="interfac web complet" src="https://github.com/user-attachments/assets/1ae09678-5b87-45f3-b428-c48a89d06948" />
<img width="736" height="1131" alt="predict = propre" src="https://github.com/user-attachments/assets/ce556a39-bffb-4b68-a601-3ef2e546d0bd" />


## ğŸ‘©â€ğŸ’» Auteur

**Nour Elwoujoud KhÃ©miri**  
ğŸ“§ khemirinour334@gmail.com  
ğŸ“ FacultÃ© des Sciences de Sfax â€” MastÃ¨re professionnel en CybersÃ©curitÃ© et Industrie Intelligente  
ğŸ”— [LinkedIn](https://www.linkedin.com/in/nour-elwoujoud-khemiri-0463a3209/)
